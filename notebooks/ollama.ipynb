{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb546a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce02fa-5ab3-43ca-9981-5900aa59f5ac",
   "metadata": {},
   "source": [
    "ollama pull model_name:parameter\n",
    "\n",
    "ollama serve\n",
    "\n",
    "api endpoint: localhost:11434/api/generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e67ad262-b5ca-4830-9833-96837bc28dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume the model is deployed locally\n",
    "def request_function(url, prompt):\n",
    "    '''\n",
    "    url: localhost url or url from other device\n",
    "    '''\n",
    "    data = {\n",
    "        \"model\": \"llama3.2\", \n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": 50,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    response=requests.post(url=url, json=data)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print('Error:', response.status_code, response.json())\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "324d7b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtracking Line Search (BLS) is an optimization algorithm used to minimize a function by iteratively moving along the direction of the negative gradient of the objective function. It's a popular method for finding the optimal parameters for non-linear models, such as neural networks.\n",
      "\n",
      "Here's how BLS works:\n",
      "\n",
      "1. **Initialization**: The algorithm starts with an initial estimate of the parameters (e.g., weights or biases) for the model.\n",
      "2. **Gradient computation**: Compute the negative gradient of the objective function with respect to each parameter. This gives a direction that would decrease the objective function value if moved in that direction.\n",
      "3. **Line search**: Perform a one-dimensional line search along each direction found in step 2. The goal is to find the optimal step size (t) that minimizes the objective function value at the new point.\n",
      "4. **Backtracking**: After finding the optimal step size, move the parameters by this amount and repeat steps 2-3 until convergence or a stopping criterion is met.\n",
      "\n",
      "The \"backtracking\" part of BLS comes from the fact that the algorithm iteratively moves back along the direction of the negative gradient if the step size found in the line search is not sufficient to improve the objective function value. This helps to avoid getting stuck in local minima.\n",
      "\n",
      "BLS has several advantages, including:\n",
      "\n",
      "* **No curvature assumptions**: Unlike other optimization methods, BLS doesn't require knowledge of the Hessian matrix (the second derivative of the objective function).\n",
      "* **Local minimization guarantee**: If the algorithm converges, it will find a global minimum within a certain radius.\n",
      "* **Easy implementation**: BLS is relatively simple to implement and requires minimal modifications to existing optimization algorithms.\n",
      "\n",
      "However, BLS also has some limitations:\n",
      "\n",
      "* **Slow convergence**: BLS can be slow for large models or functions with multiple local minima.\n",
      "* **Difficulty in handling non-convex functions**: BLS may struggle with non-convex objective functions that have many local minima.\n",
      "\n",
      "Overall, Backtracking Line Search is a useful optimization algorithm for minimizing non-linear objectives, especially when combined with other optimization techniques like gradient descent or stochastic gradient descent.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'What is backtracking line search?'\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "response = request_function(url, prompt)\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3567f4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding = ollama.embeddings(model=\"llama3.2:1b\", prompt=\"Hello Ollama!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae749c43-5616-4e3b-89ae-b9c7b18d8c57",
   "metadata": {},
   "source": [
    "Prompt engineering:\n",
    "\n",
    "Ideally, prompt engineering should be made such that the LLM model will output an action that is available in the action space and we can parse the output to a function call in python. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381b961-3fa8-4d4a-90e8-f2fabc3fc6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
