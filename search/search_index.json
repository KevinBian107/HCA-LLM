{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"A Light-Weight Multi-Collaboration System","text":"<p>Using Language Models' In-context Learning abilities, we are developing frameworks for multi-agent collaborations system using Role-Playing Leader-Hellucinating system, or in short, RPLH. We know that as parameter increases, performance automatically increases. However, only limited works have being down on generalizing this sort of in-context learning ability into samll parameter models. In our work, we try to address this issue. We try to design an efficient <code>light-weight</code> system that is runable directly on a laptop computer \ud83d\udcbb.</p>"},{"location":"#agents-perpective","title":"Agent's Perpective","text":"<p>Importantly, non of the agents actually see the full states of teh environmnet (where all the boxes are at and where all the targets are at), so they must communicate about what they can see and what they can do to better collaborate with each other and do the actions. The purpose of designing an HCA agent is to make an hellucinating central agent that have a little bit mroe information compare to other agents and tells what all other what it sees and what the plan should be from that agent's perspective.</p> <p>Here is an example of such partial information</p> <pre><code>- Agent[0.5, 0.5]:\n  - I am in square[0.5, 0.5],\n  - I can observe ['box_green', 'target_orange'],\n  - I can do one of the following action: ['move(box_green, square[1.5, 0.5])', 'move(box_green, square[0.5, 1.5])']\n- Agent[0.5, 1.5]:\n  - I am in square[0.5, 1.5],\n  - I can observe ['target_red', 'box_green', 'target_green', 'target_green', 'target_purple', 'target_purple'],\n  - I can do one of the following action: ['move(box_green, square[1.5, 1.5])', 'move(box_green, square[0.5, 0.5])', 'move(box_green, target_green)']\n</code></pre> <p>We hope to build agent that is caplable of Social-reasoning and expecting what the other agent should be doing. We are building a generalize world model for every single agent in this environment and hopefully moving a step closer to Level-one agent.</p>"},{"location":"#rplh-systems","title":"RPLH Systems","text":"<p>We hope to setup an systematic and modularzied way of doing multi-agent communication for less parametrized language models. We have implemented multiple instances of the RPLH system dpending on <code>vanilla</code> or <code>efficient</code> version. Here is the main breakdown of the code:</p> <p><pre><code>rplh/\n\u251c\u2500\u2500 d_efficient/\n\u2502   \u251c\u2500\u2500 memory.py\n\u2502   \u251c\u2500\u2500 rplh_inference.py\n\u251c\u2500\u2500 h_efficient/\n\u2502   \u251c\u2500\u2500 env.py\n\u2502   \u251c\u2500\u2500 execution_checker.py\n\u2502   \u251c\u2500\u2500 memory.py\n\u2502   \u251c\u2500\u2500 rplh_inference.py\n\u251c\u2500\u2500 h_vanilla/\n\u2502   \u251c\u2500\u2500 env.py\n\u2502   \u251c\u2500\u2500 execution_checker.py\n\u2502   \u251c\u2500\u2500 memory.py\n\u2502   \u251c\u2500\u2500 rplh_inference.py\n\u251c\u2500\u2500 llm/\n\u2502   \u251c\u2500\u2500 language_model.py\n\u2502   \u251c\u2500\u2500 response_model.py\n\u251c\u2500\u2500 rendering/\n\u2502   \u251c\u2500\u2500 render_conversation.py\n\u2502   \u251c\u2500\u2500 render_states.py\n\u2502   \u251c\u2500\u2500 animations.py\n\u2514\u2500\u2500 inference.py\n</code></pre> Note the following:</p> <ol> <li>There are many shared features across all different type of communication system, which we ahve modularized into the <code>llm</code> folder for both using it in the vanilla and efficient models. In addition, because of implementation differences, files such as <code>env.py</code>, <code>execution_checker.py</code>, <code>memory.py</code>, and the main inference loop <code>rplh_inference.py</code> may be different, which is why each system has its own unique implementation.</li> <li>The <code>d_efficient</code> (efficient implementation of decentralized framework) is a particular instance of the <code>h_efficient</code> system (our hallucination system) where the inference loops and memory module is adjusted.</li> <li>Both <code>h_efficient</code> and <code>h_vanilla</code> are our hallucination system with slight differences in implementation, refer to these links for more details:<ul> <li><code>h_efficient</code></li> <li><code>h_vanilla</code></li> </ul> </li> <li>We have created inference functions in each of the system to be called seperately, or an direct interface can be called by specifying args on the root folder level of rplh (<code>inference.py</code>).</li> </ol>"},{"location":"#rplh-schematic","title":"RPLH Schematic:","text":"<p>The below is a graphical overview of our system:</p> <p></p>"},{"location":"#rplh-demos","title":"RPLH Demos:","text":"<p>Here is a demo of our RPLH performing multi-agent resasoning with the first HCA (central) agent hellucinating about future steps:</p> <ul> <li>Conversation demos are here</li> <li>Actual running data are here</li> </ul>"},{"location":"api/","title":"API Calls","text":""},{"location":"api/#setting-up-inference-loop","title":"Setting Up Inference Loop","text":"<p>First, we need to install the dependencies and create a separate conda environment: <pre><code>conda env create\n</code></pre> Our environment is adapted from here</p> <p>Download SLM from here and then instantiate small large language model (14B parameters qwen model) agent by: <pre><code>ollama run qwen2.5:14b-instruct-q3_K_L\n</code></pre> This need to be done in your system terminal directly, not in VScode.</p>"},{"location":"api/#running-whole-pipeline-together","title":"Running Whole Pipeline Together","text":"<p>We can diretcly use the central running parser file to cerate the environment and tehn run the main inference loop by: <pre><code>python rplh/inference.py --module_name \"h_efficient\" --model_name \"qwen2.5:14b-instruct-q3_K_L\"\n</code></pre></p>"},{"location":"api/#create-environment-seperately","title":"Create Environment Seperately","text":"<p>Create local MoveBox environment for running (depending on the version using) by: <pre><code>python rplh/h_vanilla/env.py\n</code></pre></p> <p>Or setting up RPLH-efficient system by: <pre><code>python rplh/h_efficient/env.py\n</code></pre></p>"},{"location":"api/#inference-loops-seperately","title":"Inference Loops Seperately","text":"<p>We also descigned scripts to deirectly run each seperate system. We can run original vanilla RPLH inferene loop by: <pre><code>python rplh/h_vanilla/rplh_inference.py -- model_name \"qwen2.5:14b-instruct-q3_K_L\"\n</code></pre></p> <p>or running the efficient RPLH inferene loop by: <pre><code>python rplh/h_efficient/rplh_inference.py -- model_name \"qwen2.5:14b-instruct-q3_K_L\"\n</code></pre></p>"},{"location":"api/#visualize-solutions","title":"Visualize Solutions","text":"<p>To visualize the reasoning process by rendering the conversation that each of the agent said: <pre><code>python rplh/rendering/render_conversation.py\n</code></pre></p>"},{"location":"evaluation/testing/","title":"Testing","text":"<p>Still developing ways of evaluating the models</p>"},{"location":"system/d_efficient/","title":"RPLH-Decentralized Instance","text":"<p>The decentralized RPLH instant, or <code>d_efficient</code>, is used for comparsion and validating with our Hallucinated version of RPLH system (<code>h_efficient</code> and <code>h_vanilla</code>). The decentralized system follows the general design of RPLH system with a few key implmentation details differences:</p> <ol> <li>There is no central HCA agent nor is there a judge.</li> <li>Attitude agent still exist.</li> <li>To settle disagreement in all the loca agent, action is only executed when there are more than half of the agent that is operating in the environment (have box in it's grid and have action plan given by other agents) says \"I Agree\", else, the conversatoon continuses.</li> </ol> <p>To compare to the Hallucination RPLH system, please refer to the following:</p> <ul> <li><code>h_efficient</code></li> <li><code>h_vanilla</code></li> </ul>"},{"location":"system/h_efficient/","title":"Why Efficient-RPLH System? \ud83e\udd14","text":"<p>RPLH-efficient is the improved version of the original RPLH system where we prevent many of the fallbacks of RPLH-original hallucinating about many undpoable actions. We use the <code>instructor</code> package, which usese Fine State Machine that adjust the probability output of the language model to achieve specific format of the output. With this approach, it comes with tradeoff as well:</p> <ol> <li>Markovian memory and limited prompt<ul> <li>Languag models tends to perform better when there are limited and specific information (prompt) given to them. We designed to system to be specifically utilizing this characteristic of these models.</li> </ul> </li> <li> <p>Limited convesration ability of the LM agents, less hallucination of future plans and attitude.</p> <ul> <li>Structured format response giving only in the following fashion:</li> </ul> <pre><code>class HCA(BaseModel):\n    attitude: List[str]\n    reasoning: str\n    future_step: str\n    actions_plan: Dict\nclass Judge(BaseModel):\n    justification: str\n    actions_plan: Dict\n</code></pre> </li> <li> <p>Under this system, the judge also serve as an syntactic checker as well where it would change the wrong syntax that the HCA agent have given.</p> </li> <li>To remain the clearness of information flow and decision making, when needed to retake action, the LMs that is making the new decision would use teh original functions that the original HCA agent/Local agent/Judge uses with additional feedbacks giving through functional pass in with <code>functool.partial</code>.</li> <li>To still promote some conversation going on, only output agent (HCA and judge) uses the strict output formatter on LMs, the local_agent and attitude agent outputs strings directly and is free to have more conversations.</li> </ol>"},{"location":"system/h_efficient/#optimization-tricks","title":"Optimization Tricks","text":"<p>We used many optimization hacks:</p> <ol> <li>When an local agent does not have boxes or targets in its block or when it does not have an action provided from the HCA agent, it would not be considered as a valid conversation source as its views are limited.</li> <li>When vote counts (consecutive agreement number) surpasses half the number of agents, the HCA action is executed directly. If not consecutive, the count is re-initiated.</li> <li>If the local agent agrees the HCA decision, then no judge should be involved and the HCA action would be passed on directly (relapsed) to the next round of local agent or directly get executed.</li> <li>Instead of letting the HCA agent figuring out attitude, we limit the prompt length that can be given by assigning an attitude agent taht judges the attitude of each agent on the field with current round information only, then only this attitude information is passed to teh next round HCA agent.</li> </ol>"},{"location":"system/h_vanilla/","title":"Why Vanilla-RPLH System? \ud83e\udd14","text":"<p>RPLH-original is the full representation of our construct where no limits has been posted on the structure of the output of the Language model, which comes with tradeoff of benifits and issues:</p> <ol> <li>Have room for much more emergent behavior because teh context of the historical conversation is much richer.<ul> <li>More planning for future steps.</li> <li>More reasoning about what other agents may be doing and thinking (attitude).</li> </ul> </li> <li>More computation resources needed as prompt is longer and history is longer.</li> <li>Easier for LM to hallucinate and give unrealistic outputs (syntactic error).<ul> <li>One iconic problem is where no dictionary bracket <code>{...}</code> is given.</li> <li>There need to be more while loops for re-responding for the same condiion to get a better response, hence increasing teh compuatation loads.</li> </ul> </li> <li>Easier for LM to be effected by environmental checking prompt, resulting in some unexecutable iterations.<ul> <li>Slower convergence rate because of hallucination issue of the agent.</li> </ul> </li> </ol>"},{"location":"system/h_vanilla/#maintaining-syntactic-correctness","title":"Maintaining Syntactic Correctness","text":"<p>We have also performed a set of carefully designed engineering procedures to adjust the issues metioned above while maintaining the benifits of massive communication through conversations. LM is like an child where it needs very specific and detailed promppt for instructing it to  do things. We have designed and tested a set of prompt that is designed for the LM to maximize the freedom of communication while maintaining sytactic correctness.</p> <ol> <li>Action checker</li> <li>Json Checker</li> <li>Markovian History or No History (prevent oerwhelming information and enviornmental disturbance)<ul> <li>At each round the agent is either completely fresh-agent or one-step Markovian agent.</li> </ul> </li> <li>Careful Instruction promts</li> </ol> <p>Thus, though also can be used with smaller LLMs with really careful prompting and instruction, the vanilla RPLH version is more suitable for larger parametrized models such as GPT-4, which is the model that researcher usually use under these type of vanilla setting.</p>"}]}