{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"A Light-Weight Agetnt-Based Multi-Collaboration System","text":"<p>We hope to build agent that is caplable of social-reasoning and expecting what the other agent should be doing. We are building a generalize agent model for every single agent in this environment and hopefully moving a step closer to level-one agent.</p> <p>Using Language Models' In-context Learning abilities, we are developing frameworks for multi-agent collaborations system using Role-Playing Leader-Hallucinations system, or in short, RPLH. We know that as parameter increases, performance automatically increases. However, only limited works have being down on generalizing this sort of in-context learning ability into samll parameter models. In our work, we try to address this issue. We try to design an efficient <code>light-weight</code> system that is runable directly on a laptop Macbook or using API calls with smaller/cheaper GPT models such as GPT-4o-mini. More importantly, this system can scale up in terms of the complexity of the environment and the number of agents.</p>"},{"location":"#intelligence-in-information-data-flow","title":"Intelligence In Information &amp; Data Flow","text":"<p>We believe that the key of collaboration, or of solving issues in such partial observable and actuabtable environment layes in communication and the data/inofrmation we can extract from such communication to gradually build up a world model.</p>"},{"location":"#social-reasoning-agent","title":"Social Reasoning Agent","text":"<p>In decentralzie environment, usually each agent must communicate about what they can see and what they can do to better collaborate with each other and do the actions because of the partial observability situation. Here is an example of such partial information of describing what an specific local agent can see and do.</p> <pre><code>Agent[0.5, 0.5]:\n- I am in square[0.5, 0.5],\n- I can observe ['box_green', 'target_orange'],\n- I can do one of the following action: ['move(box_green, square[1.5, 0.5])', 'move(box_green, square[0.5, 1.5])']\n</code></pre> <p>Differently, we aer using a fully-observable environment, meaning that all agent can see everything (where all the boxes are at and where all the targets are at). Howveer, the environmnet still remains partially actuatable as there are limitations to what each agent can do. However, even with this setting, a common type of problem in usual decentralzied collaboration setting (each agent has their own perspective and doesn't just do whatever the central agent tells them to do) is that it is extremely hard for different agents to come to an agreement on the plan, especially when the number of agent scales.</p> <p>Our system propose a novel approach to solve this problem where each HCA agent try to conduct social reasoning of what the other agent would think, do, and react to it's plan and recieve actual sensory feedbacks from local agents to understand what is the problem with teh previous reasoning. Idealy, through longer iyerations, the HCA would gradually build an more correct model of other agents. This situation improves complexity even more when we have different local agent playing this role of the HCA agent, meaning that they can incorporate their attitudes into giving actions.</p>"},{"location":"#advesarial-reasoning-agent","title":"Advesarial Reasoning Agent","text":"<p>Beyond standard reasonings that LM perform in multi-agent collaboration, our system (specifically <code>h_efficient</code> and <code>d_efficient</code>) supports agent based reasoning with advesarial players in teh environment that has a different objective than all other agents in the environment. Hypothetically, under a advesarial situation (spy exist in environment), a hallucination efficient system would improve the social understanding ability, hence, convergence rate (since advesarial players would try their best to disrupt the system).</p> <p>This is particularly a hard task but an interesting one because the LM is embodied in a <code>structured</code> environment where not all observations are string based, so the HCA agent must reason from each local agent's action to deduct about what they may be thinking and whether or not they are the spy agent. Furthermore, because our system has a <code>strctured</code> environment, it can be deemed as a basecase of able to scale to some other virtual enviornment that is more complicated for social reasoning (i.e. VirtualHome).</p> <p>With more interaction, the HCA may get a better idea of who the spy is and taken action to prohibit the spys.</p>"},{"location":"#rplh-demos","title":"RPLH Demos:","text":"<p>Here is a demo of our RPLH performing multi-agent resasoning with the first HCA (central) agent hallucinations about future steps:</p> <ul> <li>Conversation demos are here</li> <li>Actual running data are here</li> </ul> <p>The rendering notebook can also be accessed here</p>"},{"location":"team/","title":"About Us","text":"<p>The RPLH system is built by three undergraduate student from UCSD in 2024. The system was presented in 2024 Fall and the presentation video can be found here</p>"},{"location":"team/#meet-the-team","title":"Meet The Team!","text":""},{"location":"team/#kaiwen-bian","title":"Kaiwen Bian","text":"<p>I am a undergraduate student at UCSD double majoring in Data Science and Cognitive Behavoral Neuroscience and minoring in Mathamatics. I am passionate about finding bridges between Data Science and Neuroscience through forms of algorithm, machine learning, and mathamatics to better understand different forms of intelligence, natural or artificial.</p> <ul> <li> Website</li> <li> GitHub</li> </ul>"},{"location":"team/#weijie-zhang","title":"Weijie Zhang","text":""},{"location":"team/#ziyu-huang","title":"Ziyu Huang","text":""},{"location":"api/api/","title":"API Calls","text":""},{"location":"api/api/#setting-up","title":"Setting Up","text":"<p>First, we need to install the dependencies and create a separate conda environment:</p> <pre><code>conda env create\n</code></pre> <p>If you want to use locally hosted Ollama, you need to download the LM from here and then instantiate small large language model (14B parameters qwen model) agent by:</p> <pre><code>ollama run qwen2.5:14b-instruct-q3_K_L\n</code></pre> <p>This need to be done in your system terminal directly, not in VScode.</p> <p>If you want to use GPT with API, you need to create your own OpenAI account and then embed your API key in your system with writing this in your <code>.bash</code> file:</p> <pre><code>export OPENAI_API_KEY = \"your api key\"\n</code></pre> <p>Run the following to update system file:</p> <pre><code>source ~/.bash_profile\n</code></pre>"},{"location":"api/api/#inference-pipeline","title":"Inference Pipeline","text":"<p>We can diretcly use the central running parser file to cerate the environment and then run the main inference loop by (the inference loop takes in argument (<code>module_name</code>, <code>model_name</code>, <code>row_num</code>, <code>column_num</code>, <code>box_num_upper_bound</code>, and <code>reasoning_model</code>)):</p> <pre><code>python rplh/inference.py --module_name \"h_efficient\" --model_name \"qwen2.5:14b-instruct-q3_K_L\" --row_num 3 --column_num 3 --box_num_upper_bound 1 --reasoning_model \"agent\"\n</code></pre> <p>Depending on the communciatioon system, We recommand to use different setup combination between systems and models, which we describe them below:</p> LM/System H_Efficient Dec_Efficient H_Vanilla Ollama-qwen \u2705 \u2705 GPT-4o-mini \u2705 \u2705 \u2705 <p>Notice that for <code>h_vanilla</code> and <code>dec_efficeint</code>, we are connecting to GPT-4o-mini's backend for our tests, but switching to ollama like other systems is doable as well. Similaerly, the reasoning models has specific setting as well:</p> Reasoning/System H_Efficient Dec_Efficient H_Vanilla Standard Reasoning \u2705 \u2705 \u2705 Agent Reasoning \u2705 <p>For agent based modeling, we ahve also made a <code>config.yaml</code> file that users can customize to defien the attitude of each agent in the environment.</p>"},{"location":"api/api/#testing-systems","title":"Testing Systems","text":"<p>Conduct evaluation using our system by:</p> <pre><code>python rplh/test.py --module_name \"h_efficient\" --model_name \"qwen2.5:14b-instruct-q3_K_L\" --num_trials 5 --box_num_upper_bound 1 --start_iter 1 --row_num 3 --column_num 3 --reasoning_model \"agent\"\n</code></pre>"},{"location":"api/api/#visualize-text-solutions","title":"Visualize Text Solutions","text":"<p>To visualize the reasoning process by rendering the conversation that each of the agent said:</p> <pre><code>python rplh/rendering/render_conversation.py\n</code></pre>"},{"location":"api/api/#customize-by-runing-seperately","title":"Customize By Runing Seperately","text":""},{"location":"api/api/#create-individual-environment","title":"Create Individual Environment","text":"<p>Create local MoveBox environment for running (depending on the version using) by:</p> <pre><code>python rplh/h_vanilla/env.py\n</code></pre> <p>Or setting up RPLH-efficient system by:</p> <pre><code>python rplh/h_efficient/env.py\n</code></pre> <p>Our environment is adapted from here</p>"},{"location":"api/api/#individual-inference-loops","title":"Individual Inference Loops","text":"<p>We also descigned scripts to deirectly run each seperate system. We can run original vanilla RPLH inferene loop by:</p> <pre><code>python rplh/h_vanilla/rplh_inference.py --model_name \"qwen2.5:14b-instruct-q3_K_L\"\n</code></pre> <p>or running the efficient RPLH inferene loop by:</p> <pre><code>python rplh/h_efficient/rplh_inference.py --model_name \"qwen2.5:14b-instruct-q3_K_L\"\n</code></pre>"},{"location":"api/prompt/","title":"Prompt Structure","text":"<p>We have different prompts for different standard and agent based reasoning in our system, which we will illustrate in this section.</p>"},{"location":"api/prompt/#standard-hca-reasoning-prompt","title":"Standard HCA Reasoning Prompt","text":""},{"location":"api/structure/","title":"System Structure","text":""},{"location":"api/structure/#rplh-systems","title":"RPLH Systems","text":"<p>We hope to setup an systematic and modularzied way of doing multi-agent communication for less parametrized language models. We have implemented multiple instances of the RPLH system dpending on <code>vanilla</code> or <code>efficient</code> version. Here is the main breakdown of the code:</p> <p><pre><code>rplh/\n\u251c\u2500\u2500 env/\n\u2502   \u2514\u2500\u2500 env.py\n\u251c\u2500\u2500 systems/\n\u2502   \u251c\u2500\u2500 d_efficient/\n\u2502   \u2502   \u251c\u2500\u2500 memory.py\n\u2502   \u2502   \u2514\u2500\u2500 rplh_inference.py\n\u2502   \u251c\u2500\u2500 h_efficient/\n\u2502   \u2502   \u251c\u2500\u2500 execution_checker.py\n\u2502   \u2502   \u251c\u2500\u2500 memory/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 memory_standard.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 memory_agent.py\n\u2502   \u2502   \u251c\u2500\u2500 rplh_inference.py\n\u2502   \u2502   \u2514\u2500\u2500 rplh_agent_inference.py\n\u2502   \u251c\u2500\u2500 h_vanilla/\n\u2502   \u2502   \u251c\u2500\u2500 execution_checker.py\n\u2502   \u2502   \u251c\u2500\u2500 memory.py\n\u2502   \u2502   \u2514\u2500\u2500 rplh_inference.py\n\u251c\u2500\u2500 llm/\n\u2502   \u251c\u2500\u2500 language_model.py\n\u2502   \u2514\u2500\u2500 response_model.py\n\u251c\u2500\u2500 rendering/\n\u2502   \u251c\u2500\u2500 render_conversation.py\n\u2502   \u251c\u2500\u2500 render_states.py\n\u2502   \u2514\u2500\u2500 animations.py\n\u251c\u2500\u2500 evaluation/\n\u2502   \u2514\u2500\u2500 evaluation.py\n\u251c\u2500\u2500 test.py\n\u2514\u2500\u2500 inference.py\n</code></pre> Note the following:</p> <ol> <li>There are many shared features across all different type of communication system, which we ahve modularized into the <code>llm</code> folder, <code>env</code> folder, or the <code>rendeiring</code> folder for both using it in the vanilla and efficient models. In addition, because of implementation differences, files such as <code>execution_checker.py</code>, <code>memory.py</code>, and the main inference loop <code>rplh_inference.py</code> may be different, which is why each system has its own unique implementation.</li> <li>The <code>d_efficient</code> (efficient implementation of decentralized framework) is a particular instance of the <code>h_efficient</code> system (our hallucination system) where the inference loops and memory module is adjusted.</li> <li>Both <code>h_efficient</code> and <code>h_vanilla</code> are our hallucination system with slight differences in implementation, refer to these links for more details:<ul> <li><code>h_efficient</code></li> <li><code>h_vanilla</code></li> </ul> </li> <li>We have created inference functions in each of the system to be called seperately, or an direct interface can be called by specifying args on the root folder level of rplh (<code>inference.py</code>).</li> </ol>"},{"location":"evaluation/testing/","title":"Numerical Evaluation","text":"<p>Comparing performances of different communication system through a numerical comparison (given the same <code>seed</code>, running on smaller scaled environment, 2x2 grid with max 5 boxes/targets in total (optimality depends on seed)):</p> <p>All evaluations in this system are down using <code>gpt-40-mini</code> for speed purpose and with <code>agent</code> argument for testing reasoning abilities.</p>"},{"location":"evaluation/testing/#human-annotation","title":"Human Annotation","text":"<p>Evaluate whether the system outputs reasonable/optimal response at each steps.</p> <ol> <li>Manually reason what is the optimal number of steps given the environment.</li> <li>Given the same <code>seed</code> (same environment setup), how many boxes are removed at each step and whether this step is efficient (human-annotate).</li> <li>Does output reasoning (tokens) makes sense.</li> <li>Does action decision make snese.</li> <li>Look at one iteration lag, the <code>agent_model</code> that HCA proposed and the actual <code>actual_model</code> that local agent responses to.</li> </ol>"},{"location":"evaluation/testing/#computation-system-metrics","title":"Computation System Metrics","text":"<ol> <li>Test Success Rate (does the system converge before max number of stesp reached).</li> <li>Differences with optimla human plans</li> <li>Convergence speed (number of environmental execution steps needed for convergence (solve the given problem)).</li> </ol>"},{"location":"evaluation/testing/#agent-social-reasoning-metrics","title":"Agent Social Reasoning Metrics","text":"<ol> <li>Social understanding (similarity score in agent_model + success rate of finding spys)</li> <li>Convergence rate (Iiteration numbers)</li> <li>Correlation between social understanding and convergence rate (t-distribution + bootstrap)</li> </ol>"},{"location":"system/d_efficient/","title":"RPLH-Decentralized Instance","text":"<p>The decentralized RPLH instant, or <code>d_efficient</code>, is used for comparsion and validating with our Hallucinated version of RPLH system (<code>h_efficient</code> and <code>h_vanilla</code>). The decentralized system follows the general design of RPLH system with a few key implmentation details differences:</p> <ol> <li>There is no central HCA agent nor is there a judge.</li> <li>Attitude agent still exist.</li> <li>To settle disagreement in all the loca agent, action is only executed when there are more than half of the agent that is operating in the environment (have box in it's grid and have action plan given by other agents) says \"I Agree\", else, the conversatoon continuses.</li> </ol> <p>To compare to the Hallucination RPLH system, please refer to the following:</p> <ul> <li><code>h_efficient</code></li> <li><code>h_vanilla</code></li> </ul> <p>Since decentralzied system usually takes long time to solve the communication conflict, we mainly tested our system using GPT-4o.</p>"},{"location":"system/h_efficient/","title":"Why Efficient-RPLH System? \ud83e\udd14","text":"<p>RPLH-efficient is the improved version of the original RPLH system where we prevent many of the fallbacks of RPLH-original hallucinating about many undpoable actions. We use the <code>instructor</code> package, which usese Fine State Machine that adjust the probability output of the language model to achieve specific format of the output. With this approach, it comes with tradeoff as well:</p> <ol> <li>Markovian memory and limited prompt<ul> <li>Languag models tends to perform better when there are limited and specific information (prompt) given to them. We designed to system to be specifically utilizing this characteristic of these models.</li> </ul> </li> <li> <p>Limited convesration ability of the LM agents, less hallucination of future plans and attitude.</p> <ul> <li>Structured format response giving only in the following fashion:</li> </ul> <pre><code>class HCA(BaseModel):\n    attitude: List[str]\n    reasoning: str\n    future_step: str\n    actions_plan: Dict\nclass Judge(BaseModel):\n    justification: str\n    actions_plan: Dict\n</code></pre> </li> <li> <p>Under this system, the judge also serve as an syntactic checker as well where it would change the wrong syntax that the HCA agent have given.</p> </li> <li>To remain the clearness of information flow and decision making, when needed to retake action, the LMs that is making the new decision would use teh original functions that the original HCA agent/Local agent/Judge uses with additional feedbacks giving through functional pass in with <code>functool.partial</code>.</li> <li>To still promote some conversation going on, only output agent (HCA and judge) uses the strict output formatter on LMs, the local_agent and attitude agent outputs strings directly and is free to have more conversations.</li> <li>Extending standard reasoning, we can bake in agent based reasoning because of the structured output.</li> </ol>"},{"location":"system/h_efficient/#optimization-tricks","title":"Optimization Tricks","text":"<p>We used many optimization hacks:</p> <ol> <li>When an local agent does not have boxes or targets in its block or when it does not have an action provided from the HCA agent, it would not be considered as a valid conversation source as its views are limited.</li> <li>When vote counts (consecutive agreement number) surpasses half the number of agents, the HCA action is executed directly. If not consecutive, the count is re-initiated.</li> <li>If the local agent agrees the HCA decision, then no judge should be involved and the HCA action would be passed on directly (relapsed) to the next round of local agent or directly get executed.</li> <li>Instead of letting the HCA agent figuring out attitude, we limit the prompt length that can be given by assigning an attitude agent taht judges the attitude of each agent on the field with current round information only, then only this attitude information is passed to teh next round HCA agent.</li> </ol> <p>Because of these optimization tricks, it gets really quick in later trials for execution to happen.</p>"},{"location":"system/h_efficient/#advesarial-reasoning-system","title":"Advesarial Reasoning System","text":"<p>To perform agent based reasoning, there is few game rules that we have implemented for this advesarial environment: 1. There exist extra information in the response field, namely <code>agent_model</code> and <code>spy_model</code> to capture LM's thoughts on explictly building an agent reasoning system:     - These models ar incrementally imrpoved by teh judge HCA, not the HCA that is giving command.</p> <pre><code>class HCA_AgentModel(HCA):\n    agent_model: Dict[str, str]\n    spy_model: Dict[str, str]\n</code></pre> <ol> <li> <p>Each <code>agent_model</code> and <code>spy_model</code> are two degree Markovian (though this can be a hyperparameter) where each judege HCA when updating the models can see the previous two versions of the models.</p> </li> <li> <p>For the speedyness of our system, once a spy does not have boxes to move or tragets to match in its grid, then the system will remaind the HCA that one spy agent has been identifided. If all the spy agent has been identified, the system would remaind the HCA agent to not look for spy anymore.</p> </li> </ol>"},{"location":"system/h_vanilla/","title":"Why Vanilla-RPLH System? \ud83e\udd14","text":"<p>RPLH-vanilla is the vanilla version of our construct where no limits has been posted on the structure of the output of the Language model, which comes with tradeoff of benifits and issues:</p> <ol> <li>Have room for much more emergent behavior because the context of the historical conversation is much richer.<ul> <li>More planning for future steps.</li> <li>More reasoning about what other agents may be doing and thinking (attitude).</li> </ul> </li> <li>More computation resources needed as prompt is longer and history is longer.</li> <li>Easier for LM to hallucinate and give unrealistic outputs (syntactic error).<ul> <li>One iconic problem is where no dictionary bracket <code>{...}</code> is given.</li> <li>There need to be more while loops for re-responding for the same condiion to get a better response, hence increasing the compuatation loads.</li> </ul> </li> <li>Easier for LM to be effected by environmental checking prompt, resulting in some unexecutable iterations.<ul> <li>Slower convergence rate because of hallucination issue of the agent.</li> </ul> </li> <li>Only include standard reasoning ability with no agent reasoning baked in since the output of the model is hard to control.</li> </ol>"},{"location":"system/h_vanilla/#maintaining-syntactic-correctness","title":"Maintaining Syntactic Correctness","text":"<p>We have also performed a set of carefully designed engineering procedures to adjust the issues metioned above while maintaining the benifits of massive communication through conversations. LM is like an child where it needs very specific and detailed promppt for instructing it to  do things. We have designed and tested a set of prompt that is designed for the LM to maximize the freedom of communication while maintaining sytactic correctness.</p> <ol> <li>Action checker</li> <li>Json Checker</li> <li>Markovian History or No History (prevent oerwhelming information and enviornmental disturbance)<ul> <li>At each round the agent is either completely fresh-agent or one-step Markovian agent.</li> </ul> </li> <li>Careful Instruction promts</li> </ol> <p>Thus, though also can be used with smaller LLMs with really careful prompting and instruction, the vanilla RPLH version is more suitable for larger parametrized models such as GPT-4, which is the model that researcher usually use under these type of vanilla setting.</p>"}]}